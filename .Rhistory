amat_group <- matrix(0, nrow = length(names(freq_featGroup)), ncol = length(names(freq_featGroup)), dimnames = list(names(freq_featGroup),names(freq_featGroup))) # Empty adjacency matrix
for(i in 1:length(allRuleSets)){ # for ith rule set
ruleSet <- allRuleSets[[i]]
for(j in 1:length(ruleSet)){# for jth rule
rule <- ruleSet[[j]]
# Check that there are more than 1 rule
if(length(rule) > 1){
# Connections between rules
for(k in 1:(length(rule)-1)){ # for kth feature in rule, up to 2nd to last feature
feat1 <- strsplit(rule[k], "_")[[1]][1]
for(l in (k+1):length(rule)){ # for each rule after the kth rule
feat2 <- strsplit(rule[l], "_")[[1]][1]
# Order by frequency (link goes from most to least frequent; alphabetical if even frequency)
if(freq_featGroup[feat1] > freq_featGroup[feat2]){
feats <- c(feat1, feat2)
} else if(freq_featGroup[feat2] > freq_featGroup[feat1]){
feats <- c(feat2, feat1)
} else{
feats <- sort(c(feat1, feat2))
}
amat_group[feats[1], feats[2]] <- amat_group[feats[1], feats[2]] + 1
#amat_group[feat2, feat1] <- amat_group[feat2, feat1] + 1 # Comment out to eliminate duplicate entries
}
}
# if only 1 rule, then increment entry w/ itself as row and col
} else {
feat1 <- strsplit(rule[1], "_")[[1]][1]
amat_group[feat1, feat1] <- amat_group[feat1, feat1] + 1
}
}
}
allRuleSets <- out[[1]]
# find most frequent rules of each length
byLen <- vector(mode="list", length=maxLen) # rules grouped by length
for(i in 1:length(allRuleSets)){ # loop through all rule sets
ruleSet <- allRuleSets[[i]]
for(j in 1:length(ruleSet)){ # loop through all rules in rule set
rule <- sort(ruleSet[[j]]) # sort alphabetically
if(simplify){
for(k in 1:length(rule)){ # loop through each condition in the rule
rule[k] <- simplifyCondition(rule[k], opp, allFeatures) # simplify if possible
}
}
len <- length(rule)
byLen[[len]] <- rbind(byLen[[len]], rule) # append to group of rules of same length
}
}
byLen
simplify <- T
# find most frequent rules of each length
byLen <- vector(mode="list", length=maxLen) # rules grouped by length
for(i in 1:length(allRuleSets)){ # loop through all rule sets
ruleSet <- allRuleSets[[i]]
for(j in 1:length(ruleSet)){ # loop through all rules in rule set
rule <- sort(ruleSet[[j]]) # sort alphabetically
if(simplify){
for(k in 1:length(rule)){ # loop through each condition in the rule
rule[k] <- simplifyCondition(rule[k], opp, allFeatures) # simplify if possible
}
}
len <- length(rule)
byLen[[len]] <- rbind(byLen[[len]], rule) # append to group of rules of same length
}
}
## Helper functions
#' Function that simplifies feature if possible
#' @param originalFeature feature to simplify
#' @param featureNames vector of all feature names, repeated according to multiplicity
#' @param featureValues vector of all values corresponding to features (same order as featureNames)
#' @return name of feature to use
#' Original feature if no _neg suffix
#' if _neg suffix: original feature and value if none or multiple alternative values, other feature value if exactly one alternative
simplifyFeature <- function(originalFeature, featureNames, featureValues){
split_feat <- strsplit(originalFeature, "_")[[1]]
name <- split_feat[1]
value <- split_feat[2]
# Check if _neg and if there is exactly one other possible value
if(tail(split_feat, 1) == "neg" & (sum(featureNames==name) == 2)){
# Return other feature and value
return(paste(name, featureValues[featureNames==name & featureValues!=value], sep="_"))
}
else {
return(originalFeature)
}
}
#'
simplifyCondition <- function(cond, opp, feats){
split <- strsplit(cond, "_")[[1]]
if (tail(split, 1) != "neg") {  # if not negation, then return original
return(cond)
} else {
fg <- split[1]
val <- split[length(split)-1]
ind <- which(opp==val, arr.ind = T)
if (length(ind) > 0) {
for (i in 1:nrow(ind)) {
row <- ind[i, 1]
col <- ifelse(ind[i, 2]==1, yes=2, no=1)  # other column is opposite value
feat <- paste(fg, opp[row, col], sep="_")
if (feat %in% feats) {  # check if this opposite value is one of the possible features
return(feat)
}
}
}
}
return(cond)  # not one of the possible features
}
#' Get labels for features
#'
#' Get the labels of a features to use in graphs
#'
#' @param feat feature or vector of features to get the label of; should be of the form "feature_value"
#' @param labels_df dataframe of unique feature and corresponding labels; first column is the feature, second column is corresponding label
#' @param neg_label prefix to use for negative features
#' @return label to use for feat
getLabel <- function(feat, labels_df, neg_label){
label <- c()
for(i in 1:length(feat)){
neg <- ""
split_feat <- strsplit(feat[i], "_")[[1]]
if ( tail(split_feat, 1) == "neg" ){
neg <- neg_label
feat[i] <- paste(split_feat[1:(length(split_feat)-1)], collapse="_")
}
label[i] <- paste(neg, labels_df[labels_df[,1]==feat[i],2], sep="")
}
return(label)
}
#' Get the number of cases that satisfy a rule
#'
#' @param rule rule to check: must be in the format feature_value or feature_value_neg
#' @param df data to check
#' @param Y if included, checks that case's Y=y_val
#' @param y_val value Y should take
#' @return number of cases in data that satisfies rule
numCases <- function(rule, df, Y=NULL, y_val=1){
feats <- c() # Features to check
values <- c() # Values of features
for(i in 1:length(rule)){ # Loops through all features in rule to get feature (w/o _neg) and value
feats[i] <- rule[i]
values[i] <- 1
split_feat <- strsplit(feats[i], "_")[[1]]
if ( tail(split_feat, 1) == "neg" ) {
#if(length(split_feat) == 3){ # Checks if _neg
values[i] <- 0
feats[i] <- paste(split_feat[1:(length(split_feat)-1)], collapse="_")
}
}
if(is.null(Y)){
val <- sum(apply(df[,feats] == matrix(rep(values, nrow(df)), ncol=length(values), byrow=T), 1, all)) # Number of cases that satisfy rule
} else { # must satisfy Y
val <- sum(apply(cbind(Y, df[,feats]) == matrix(rep(c(y_val,values), nrow(df)), ncol=(length(values)+1), byrow=T), 1, all)) # Number of cases that satisfy rule and Y
}
return(val)
}
#'
getFeat <- function(rule) {
split_feat <- strsplit(rule, "_")[[1]]
feat <- ifelse(tail(split_feat, 1) == "neg",
yes=paste(split_feat[1:(length(split_feat)-1)], collapse="_"),
no=paste(split_feat, collapse="_"))
return(feat)
}
# find most frequent rules of each length
byLen <- vector(mode="list", length=maxLen) # rules grouped by length
for(i in 1:length(allRuleSets)){ # loop through all rule sets
ruleSet <- allRuleSets[[i]]
for(j in 1:length(ruleSet)){ # loop through all rules in rule set
rule <- sort(ruleSet[[j]]) # sort alphabetically
if(simplify){
for(k in 1:length(rule)){ # loop through each condition in the rule
rule[k] <- simplifyCondition(rule[k], opp, allFeatures) # simplify if possible
}
}
len <- length(rule)
byLen[[len]] <- rbind(byLen[[len]], rule) # append to group of rules of same length
}
}
byLen
rule
rule[k]
# bootstrapped ---------------------
load("sherlock/out/out_vote_sherlock.rda")
allRuleSets <- out[[1]]
# find most frequent rules of each length
byLen <- vector(mode="list", length=maxLen) # rules grouped by length
for(i in 1:length(allRuleSets)){ # loop through all rule sets
ruleSet <- allRuleSets[[i]]
for(j in 1:length(ruleSet)){ # loop through all rules in rule set
rule <- sort(ruleSet[[j]]) # sort alphabetically
if(simplify){
for(k in 1:length(rule)){ # loop through each condition in the rule
rule[k] <- simplifyCondition(rule[k], opp, allFeatures) # simplify if possible
}
}
len <- length(rule)
byLen[[len]] <- rbind(byLen[[len]], rule) # append to group of rules of same length
}
}
byLen
# get top maxRules rules of each length
rules <- vector(mode="list", length=maxLen) ## names of the most frequent rules
freq <- vector(mode="list", length=maxLen) ## frequencies of the most frequent rules
reps <- length(allIndices)
for(len in 1:maxLen){
if (!is.null(byLen[[len]])) {
temp <- as.data.frame(byLen[[len]]) %>% dplyr::group_by_all() %>% dplyr::tally(sort=T) ## group together all columns and count number of times each row (rule) appears
thisFreq <- c(temp[,len+1]/reps)[[1]] # frequency as a proporiton of number of bootstraps
keep <- (thisFreq >= minProp)  # indices of rules that appear sufficiently many time
if ( sum(keep) > 0 ) {
rules[[len]] <- as.data.frame(temp[keep,1:len][1:min(maxRules, sum(keep)),])
freq[[len]] <- thisFreq[keep][1:min(maxRules, sum(keep))]
}
}
}
require(dplyr)
allIndices <- lapply(1:rep, function(x) sample(nrow(df), nrow(df), replace=T))
df <- X
allIndices <- lapply(1:rep, function(x) sample(nrow(df), nrow(df), replace=T))
df
rep <- 100
if ( length(allIndices) == 0 ) {
allIndices <- lapply(1:rep, function(x) sample(nrow(df), nrow(df), replace=T))
}
allIndices <- lapply(1:rep, function(x) sample(nrow(df), nrow(df), replace=T))
# get top maxRules rules of each length
rules <- vector(mode="list", length=maxLen) ## names of the most frequent rules
freq <- vector(mode="list", length=maxLen) ## frequencies of the most frequent rules
reps <- length(allIndices)
length(allRuleSets)
reps <- length(allRuleSets)
for(len in 1:maxLen){
if (!is.null(byLen[[len]])) {
temp <- as.data.frame(byLen[[len]]) %>% dplyr::group_by_all() %>% dplyr::tally(sort=T) ## group together all columns and count number of times each row (rule) appears
thisFreq <- c(temp[,len+1]/reps)[[1]] # frequency as a proporiton of number of bootstraps
keep <- (thisFreq >= minProp)  # indices of rules that appear sufficiently many time
if ( sum(keep) > 0 ) {
rules[[len]] <- as.data.frame(temp[keep,1:len][1:min(maxRules, sum(keep)),])
freq[[len]] <- thisFreq[keep][1:min(maxRules, sum(keep))]
}
}
}
maxRules <- 10
for(len in 1:maxLen){
if (!is.null(byLen[[len]])) {
temp <- as.data.frame(byLen[[len]]) %>% dplyr::group_by_all() %>% dplyr::tally(sort=T) ## group together all columns and count number of times each row (rule) appears
thisFreq <- c(temp[,len+1]/reps)[[1]] # frequency as a proporiton of number of bootstraps
keep <- (thisFreq >= minProp)  # indices of rules that appear sufficiently many time
if ( sum(keep) > 0 ) {
rules[[len]] <- as.data.frame(temp[keep,1:len][1:min(maxRules, sum(keep)),])
freq[[len]] <- thisFreq[keep][1:min(maxRules, sum(keep))]
}
}
}
rules
minProp <- .05
for(len in 1:maxLen){
if (!is.null(byLen[[len]])) {
temp <- as.data.frame(byLen[[len]]) %>% dplyr::group_by_all() %>% dplyr::tally(sort=T) ## group together all columns and count number of times each row (rule) appears
thisFreq <- c(temp[,len+1]/reps)[[1]] # frequency as a proporiton of number of bootstraps
keep <- (thisFreq >= minProp)  # indices of rules that appear sufficiently many time
if ( sum(keep) > 0 ) {
rules[[len]] <- as.data.frame(temp[keep,1:len][1:min(maxRules, sum(keep)),])
freq[[len]] <- thisFreq[keep][1:min(maxRules, sum(keep))]
}
}
}
rules
# calculate coverage statistics
tp <- vector(mode="list", length=maxLen)
fp <- vector(mode="list", length=maxLen)
for(len in 1:maxLen){
if(!is.null(rules[[len]]) && nrow(rules[[len]])>0){
tp[[len]] <- .getTP(rules=rules[[len]], allIndices=allIndices, reps=reps, df=df, Y=Y)
fp[[len]] <- .getFP(rules=rules[[len]], allIndices=allIndices, reps=reps, df=df, Y=Y)
}
}
#' Get proportion of true positives
.getTP <- function(rules, allIndices, reps, df, Y){
stats <- c()
for(i in 1:nrow(rules)){ # Loop through rules
coverage <- c()
for(j in 1:reps){ # Loop through bootstrap samples
ind <- allIndices[[j]]
coverage[j] <- numCases(rule=unlist(rules[i,]), df=df[ind,], Y=Y[ind]) # number of true positives in that bootstrap
}
stats <- rbind(stats,
c(quantile(coverage, .025), median(coverage), quantile(coverage, .975))/length(Y))
}
colnames(stats) <- c("min", "median", "max") # min=.025 quantile, max=.975 quantile
return(data.frame(stats))
}
#' Get proportion of false positives
.getFP <- function(rules, allIndices, reps, df, Y){
stats <- c()
for(i in 1:nrow(rules)){ # Loop through rules
coverage <- c()
for(j in 1:reps){ # Loop through bootstrap samples
ind <- allIndices[[j]]
coverage[j] <- numCases(rule=unlist(rules[i,]), df=df[ind,], Y=Y[ind], y_val=0) # number of false positives in that bootstrap
}
stats <- rbind(stats,
c(quantile(coverage, .025), median(coverage), quantile(coverage, .975))/length(Y))
}
colnames(stats) <- c("min", "median", "max") # min=.025 quantile, max=.975 quantile
return(data.frame(stats))
}
# calculate coverage statistics
tp <- vector(mode="list", length=maxLen)
fp <- vector(mode="list", length=maxLen)
for(len in 1:maxLen){
if(!is.null(rules[[len]]) && nrow(rules[[len]])>0){
tp[[len]] <- .getTP(rules=rules[[len]], allIndices=allIndices, reps=reps, df=df, Y=Y)
fp[[len]] <- .getFP(rules=rules[[len]], allIndices=allIndices, reps=reps, df=df, Y=Y)
}
}
library(BRS)
library(BRS)
library(BRS)
library(BRS)
library(BRS)
library(BRS)
library(BRS)
library(BRS)
?BRS
??BRS
devtools::documents()
install.packages("devtools")
devtools::documents()
devtools::document()
require(devtools)
install()
??agg
??BRS::agg
??BRS
require(BRS)
??agg_BRS
getwd()
install_github("albertchiu/brs")
devtools::install_github("albertchiu/brs")
devtools::install_github("albert-chiu/brs")
library(brs)
reticulate::conda_install(envname = "BRS_conda",
conda="auto",  # check default sequence of locations for conda
packages = c("numpy", "pandas", "scikit-learn", "scipy"),
python_version = "3.6.1")
## Set working directory -----------------------------
rep_path <- "~/Dropbox/QCAplus/replication_JoP/replication/"  # set this to be the replication folder
setwd(rep_path)
## Install BRS function from source and load python ---------------------------
# make sure you have all dependencies installed first
install.packages(c("reticulate", "circlize", "Rtsne"))
install.packages(c("reticulate", "circlize", "Rtsne"))
# install BRS
install.packages("BRS_0.0.0.9006.tar.gz", repos = NULL, type="source")
# Python
reticulate::conda_install(envname = "BRS_conda",
conda="auto",  # check default sequence of locations for conda
packages = c("numpy", "pandas", "scikit-learn", "scipy"),
python_version = "3.6.1")
# install BRS
install.packages("BRS_0.0.0.9006.tar.gz", repos = NULL, type="source")
getwd()
## Set working directory -----------------------------
rep_path <- "~/Dropbox/QCAplus/replication_JoP/replication/"  # set this to be the replication folder
setwd(rep_path)
reticulate::conda_create("BRS_conda")
reticulate::conda_install(envname = "BRS_conda",
conda="auto",  # check default sequence of locations for conda
packages = c("numpy", "pandas", "scikit-learn", "scipy"),
python_version = "3.6.1")
reticulate::conda_install(envname = "BRS_conda",
conda="auto",  # check default sequence of locations for conda
packages = c("numpy", "pandas", "scikit-learn", "scipy"),
python_version = "3.6")
set.seed(123)
data("lipset_df", "lipset_Y")
library(brs)
data("lipset_df", "lipset_Y")
out_lipset <- brs::BRS(df = lipset_df, Y = lipset_Y,
maxLen=maxLen, trainProp=trainProp,
numIter=numIter, numChain=numChain,
supp=supp, numMine=numMine,
alpha_1=alpha_1, alpha_2=alpha_2,
beta_1=beta_1, beta_2=beta_2,
prior_type="poisson",
alpha_l=NULL, beta_l=NULL,  # not used for the poisson version
lambda=lambda, nu=nu,    # note: BRS_0.0.0.9006 uses nu to refer to eta from the paper
bootstrap=T, reps=100L,
print=F,
seed=sample.int(1e5, size=1))
# note: replication code for paper uses 3.6.1,
# but version names have since been truncated
reticulate::use_condaenv("BRS_conda")  ## use this conda environment for BRS
out_lipset <- brs::BRS(df = lipset_df, Y = lipset_Y,
maxLen=maxLen, trainProp=trainProp,
numIter=numIter, numChain=numChain,
supp=supp, numMine=numMine,
alpha_1=alpha_1, alpha_2=alpha_2,
beta_1=beta_1, beta_2=beta_2,
prior_type="poisson",
alpha_l=NULL, beta_l=NULL,  # not used for the poisson version
lambda=lambda, nu=nu,    # note: BRS_0.0.0.9006 uses nu to refer to eta from the paper
bootstrap=T, reps=100L,
print=F,
seed=sample.int(1e5, size=1))
?reticulate::conda_install
# install python and packages to environment
reticulate::conda_install(envname = "BRS_conda",
conda="auto",  # check default sequence of locations for conda
channel = "osx-arm64",
packages = c("numpy", "pandas", "scikit-learn", "scipy"),
python_version = "3.6")  # python version to use
# note: replication code for paper uses 3.6.1,
# but version names have since been truncated
reticulate::use_condaenv("BRS_conda")  ## use this conda environment for BRS
# note: replication code for paper uses 3.6.1,
# but version names have since been truncated
reticulate::use_condaenv("BRS_conda")  ## use this conda environment for BRS
set.seed(123)
set.seed(123)
data("lipset_df", "lipset_Y")
out_lipset <- brs::BRS(df = lipset_df, Y = lipset_Y,
maxLen=maxLen, trainProp=trainProp,
numIter=numIter, numChain=numChain,
supp=supp, numMine=numMine,
alpha_1=alpha_1, alpha_2=alpha_2,
beta_1=beta_1, beta_2=beta_2,
prior_type="poisson",
alpha_l=NULL, beta_l=NULL,  # not used for the poisson version
lambda=lambda, nu=nu,    # note: BRS_0.0.0.9006 uses nu to refer to eta from the paper
bootstrap=T, reps=100L,
print=F,
seed=sample.int(1e5, size=1))
# install python and packages to environment
reticulate::conda_install(envname = "BRS_conda",
conda="auto",  # check default sequence of locations for conda
channel = "osx-arm64e",
packages = c("numpy", "pandas", "scikit-learn", "scipy"),
python_version = "3.6")  # python version to use
# install python and packages to environment
reticulate::conda_install(envname = "BRS_conda",
conda="auto",  # check default sequence of locations for conda
channel = "osx-arm64",
packages = c("numpy", "pandas", "scikit-learn", "scipy"),
python_version = "3.6")  # python version to use
# install python and packages to environment
reticulate::conda_install(envname = "BRS_conda",
conda="auto",  # check default sequence of locations for conda
channel = "conda-forge/osx-arm64",
packages = c("numpy", "pandas", "scikit-learn", "scipy"),
python_version = "3.6")  # python version to use
# note: replication code for paper uses 3.6.1,
# but version names have since been truncated
reticulate::use_condaenv("BRS_conda")  ## use this conda environment for BRS
set.seed(123)
data("lipset_df", "lipset_Y")
out_lipset <- brs::BRS(df = lipset_df, Y = lipset_Y,
maxLen=maxLen, trainProp=trainProp,
numIter=numIter, numChain=numChain,
supp=supp, numMine=numMine,
alpha_1=alpha_1, alpha_2=alpha_2,
beta_1=beta_1, beta_2=beta_2,
prior_type="poisson",
alpha_l=NULL, beta_l=NULL,  # not used for the poisson version
lambda=lambda, nu=nu,    # note: BRS_0.0.0.9006 uses nu to refer to eta from the paper
bootstrap=T, reps=100L,
print=F,
seed=sample.int(1e5, size=1))
reticulate::conda_remove(BRS_conda")
reticulate::conda_remove("BRS_conda")
# create conda environment
reticulate::conda_create("BRS_conda")
# install python and packages to environment
reticulate::conda_install(envname = "BRS_conda",
conda="auto",  # check default sequence of locations for conda
channel = "conda-forge/osx-arm64e",
packages = c("numpy", "pandas", "scikit-learn", "scipy"),
python_version = "3.6")  # python version to use
# install python and packages to environment
reticulate::conda_install(envname = "BRS_conda",
conda="auto",  # check default sequence of locations for conda
channel = "conda-forge/osx-arm64",
packages = c("numpy", "pandas", "scikit-learn", "scipy"),
python_version = "3.6")  # python version to use
# note: replication code for paper uses 3.6.1,
# but version names have since been truncated
reticulate::use_condaenv("BRS_conda")  ## use this conda environment for BRS
# note: replication code for paper uses 3.6.1,
# but version names have since been truncated
reticulate::use_condaenv(envname="BRS_conda")  ## use this conda environment for BRS
?reticulate::use_condaenv
# note: replication code for paper uses 3.6.1,
# but version names have since been truncated
reticulate::use_condaenv(condaenv="BRS_conda")  ## use this conda environment for BRS
set.seed(123)
data("lipset_df", "lipset_Y")
out_lipset <- brs::BRS(df = lipset_df, Y = lipset_Y,
maxLen=maxLen, trainProp=trainProp,
numIter=numIter, numChain=numChain,
supp=supp, numMine=numMine,
alpha_1=alpha_1, alpha_2=alpha_2,
beta_1=beta_1, beta_2=beta_2,
prior_type="poisson",
alpha_l=NULL, beta_l=NULL,  # not used for the poisson version
lambda=lambda, nu=nu,    # note: BRS_0.0.0.9006 uses nu to refer to eta from the paper
bootstrap=T, reps=100L,
print=F,
seed=sample.int(1e5, size=1))
getwd()
# create conda environment
reticulate::conda_create("BRS_conda")
